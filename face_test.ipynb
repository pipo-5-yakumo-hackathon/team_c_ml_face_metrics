{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1753081323.492763 60367568 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M3 Max\n",
      "W0000 00:00:1753081323.495292 60572273 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753081323.500149 60572271 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: age: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 23.00it/s]\n",
      "Action: age: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 22.71it/s]\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from deepface import DeepFace\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "def extract_skin_mask(image):\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(rgb_image)\n",
    "    mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            landmarks = []\n",
    "            for idx in range(234, 454):\n",
    "                x = int(face_landmarks.landmark[idx].x * image.shape[1])\n",
    "                y = int(face_landmarks.landmark[idx].y * image.shape[0])\n",
    "                landmarks.append([x, y])\n",
    "            hull = cv2.convexHull(np.array(landmarks))\n",
    "            cv2.drawContours(mask, [hull], -1, 255, -1)\n",
    "    mask = cv2.erode(mask, np.ones((5, 5), np.uint8), iterations=1)\n",
    "    return mask, results.multi_face_landmarks\n",
    "\n",
    "def draw_face_landmarks(image, face_landmarks):\n",
    "    annotated_image = image.copy()\n",
    "    if face_landmarks:\n",
    "        for landmarks in face_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image=annotated_image,\n",
    "                landmark_list=landmarks,\n",
    "                connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                landmark_drawing_spec=None,\n",
    "                connection_drawing_spec=mp_drawing_styles\n",
    "                .get_default_face_mesh_tesselation_style())\n",
    "            \n",
    "            mp_drawing.draw_landmarks(\n",
    "                image=annotated_image,\n",
    "                landmark_list=landmarks,\n",
    "                connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "                landmark_drawing_spec=None,\n",
    "                connection_drawing_spec=mp_drawing_styles\n",
    "                .get_default_face_mesh_contours_style())\n",
    "    return annotated_image\n",
    "\n",
    "def plot_emotions(emotions: dict):\n",
    "    labels = list(emotions.keys())\n",
    "    values = list(emotions.values())\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    bars = ax.bar(labels, values, color='skyblue')\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.set_ylabel('Percentage (%)')\n",
    "    ax.set_title('Emotion Distribution')\n",
    "    ax.bar_label(bars, fmt='%.1f%%')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def analyze_health(image):\n",
    "    try:\n",
    "        image = cv2.cvtColor(image.astype(np.uint8), cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        result = DeepFace.analyze(img_path=image, actions=['emotion', 'age'], enforce_detection=False)\n",
    "        emotions = result[0]['emotion']\n",
    "        age = result[0]['age']\n",
    "        emotion_fig = plot_emotions(emotions)\n",
    "\n",
    "        mask, face_landmarks = extract_skin_mask(image)\n",
    "        if np.sum(mask) == 0:\n",
    "            raise ValueError(\"No face detected\")\n",
    "        \n",
    "        annotated_image = draw_face_landmarks(image, face_landmarks)\n",
    "        annotated_image = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        skin_rgb = image[mask > 0]\n",
    "        r_mean = np.mean(skin_rgb[:, 2]) if skin_rgb.size > 0 else 0\n",
    "        \n",
    "        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "        lab = lab.astype(np.float32)\n",
    "        lab[:,:,0] = lab[:,:,0] * (100/255) # L: 0-100\n",
    "        lab[:,:,1] = lab[:,:,1] - 128       # a: -128 to 127\n",
    "        lab[:,:,2] = lab[:,:,2] - 128       # b: -128 to 127\n",
    "        skin_lab = lab[mask > 0]\n",
    "        \n",
    "        l_mean = np.mean(skin_lab[:, 0]) if skin_lab.size > 0 else 0\n",
    "        a_mean = np.mean(skin_lab[:, 1]) if skin_lab.size > 0 else 0\n",
    "        b_mean = np.mean(skin_lab[:, 2]) if skin_lab.size > 0 else 0\n",
    "        \n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        contrast = gray.std()  # 0-255\n",
    "        \n",
    "        hydration = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "\n",
    "        health_report = (\n",
    "            f\"Basic Metrics:\\n\"\n",
    "            f\"â€¢ Estimated Age: {int(age)} years\\n\"\n",
    "            f\"â€¢ Skin Redness (RGB): {r_mean:.1f}/255\\n\"\n",
    "            f\"   - {'High (possible irritation)' if r_mean > 180 else 'Normal' if r_mean > 100 else 'Low'}\\n\\n\"\n",
    "            \n",
    "            f\"Medical Skin Analysis (LAB):\\n\"\n",
    "            f\"â€¢ Brightness (L): {l_mean:.1f}/100\\n\"\n",
    "            f\"   - {'Light' if l_mean > 70 else 'Normal' if l_mean > 50 else 'Dark'}\\n\"\n",
    "            f\"â€¢ Red-Green (a): {a_mean:.1f} (Norm: 5-20)\\n\"\n",
    "            f\"   - {'Redness' if a_mean > 20 else 'Normal' if a_mean > 5 else 'Pale'}\\n\"\n",
    "            f\"â€¢ Blue-Yellow (b): {b_mean:.1f} (Norm: 10-25)\\n\"\n",
    "            f\"   - {'Yellow tint' if b_mean > 30 else 'Normal' if b_mean > 10 else 'Cool'}\\n\\n\"\n",
    "            \n",
    "            f\"Skin Quality:\\n\"\n",
    "            f\"â€¢ Contrast: {contrast:.1f}/255 (Norm: 40-70)\\n\"\n",
    "            f\"   - {'Low (fatigue)' if contrast < 35 else 'Normal' if contrast <= 70 else 'High (harsh light)'}\\n\"\n",
    "            f\"â€¢ Hydration: {hydration:.1f} (Norm: 100-300)\\n\"\n",
    "            f\"   - {'Dry' if hydration < 100 else 'Normal' if hydration <= 300 else 'Well-hydrated'}\\n\\n\"\n",
    "            \n",
    "            f\"Quick Tips:\\n\"\n",
    "            f\"- Redness > 180? Check for irritation\\n\"\n",
    "            f\"- Hydration < 100? Drink more water\\n\"\n",
    "            f\"- Contrast < 35? Try better lighting\"\n",
    "        )\n",
    "\n",
    "        return annotated_image, emotion_fig, health_report\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Analysis error:\", str(e))\n",
    "        return None, None, f\"Error: {str(e)}\\nðŸ“¸ Please upload a clear front-facing photo without filters.\"\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=analyze_health,\n",
    "    inputs=gr.Image(type=\"numpy\", label=\"Upload Face Photo\", height=400),\n",
    "    outputs=[\n",
    "        gr.Image(label=\"Face Landmarks\", type=\"numpy\", height=400),\n",
    "        gr.Plot(label=\"Emotion Analysis\"),\n",
    "        gr.Textbox(label=\"Health Report\", lines=22)\n",
    "    ],\n",
    "    title=\"Face Health Analyzer\",\n",
    "    description=\"Get detailed skin and emotion analysis with clear interpretations!\",\n",
    "    allow_flagging=\"never\"\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
